# Learning Project : Large Language Models

Welcome to my Summer of Science Learning Project!  
This repository documents my 8-week journey into the world of **Large Language Models (LLMs)** — from understanding machine learning foundations to exploring advanced transformer-based architectures and applying them in real-world applications like chatbots and generative AI.

---

## What I Learned

Over the 8-week journey, I explored the fundamentals and advancements in Large Language Models (LLMs). I began with core machine learning concepts like regression and classification, followed by deep learning fundamentals including CNNs, RNNs, and optimization techniques. I then delved into natural language processing, learning about word embeddings, attention mechanisms, and transformer architectures. Midway, I studied the inner workings of transformers—like self-attention and positional encoding—and applied this knowledge to generative AI tasks such as chatbot development and prompt engineering. In the final weeks, I explored advanced LLMs like BERT, T5, and GPT-3, focusing on their architectures and ethical use, and culminating with a final video presentation.

---

## Folder Structure


├── [plan_of_action.pdf](./plan_of_action.pdf)              # Detailed weekly plan and resources used

├── [SOS Midterm Report.pdf](./SOS%20Midterm%20Report.pdf)         # Learnings and progress till mid-term

├── [SOS Endterm Report.pdf](./SOS%20Endterm%20Report.pdf)         # Final comprehensive report of the entire journey

├── [transformers presentation.pdf](./transformers%20presentation.pdf)  # Slides explaining self-attention and transformers

## Video Presentation 

This is my final video presentation of SOS, it includes a detailed explanation about self attention and the transformer architecture.

Watch the **SOS Final Video Presentation** here: 

[Click to View](https://drive.google.com/file/d/1VLJVjRsu0feTZ1XnEf1yV6TPYjZnyCbW/view?usp=sharing)
